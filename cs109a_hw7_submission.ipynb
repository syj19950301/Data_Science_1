{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 7\n",
    "# LDA/QDA and Decision Trees\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#import pydotplus\n",
    "#import io\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multiclass Thyroid Classification\n",
    "\n",
    "In this problem, you will build a model for diagnosing disorders in a patient's thyroid gland. Given the results of medical tests on a patient, the task is to classify the patient either as:\n",
    "- *normal* (class 1)\n",
    "- having *hyperthyroidism* (class 2)\n",
    "- or having *hypothyroidism* (class 3). \n",
    "\n",
    "The data set is provided in the file `hw7_dataset.csv`. Columns 1-2 contain biomarkers for a patient (predictors):\n",
    "- Biomarker 1: (Logarithm of) level of basal thyroid-stimulating hormone (TSH) as measured by radioimmuno assay\n",
    "- Biomarker 2: (Logarithm of) maximal absolute difference of TSH value after injection of 200 micro grams of thyrotropin-releasing hormone as compared to the basal value.\n",
    "\n",
    "The last column contains the diagnosis for the patient from a medical expert. This data set was obtained from the UCI machine learning repository.\n",
    "\n",
    "Notice that unlike previous exercises, the task at hand is a 3-class classification problem. We will explore the use of different methods for multiclass classification.\n",
    "\n",
    "First task: split the data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "df = pd.read_csv('hw7_dataset.csv')\n",
    "df = df.rename(columns={'Biomarker 1': 'Biomarker_1', 'Biomarker 2': 'Biomarker_2'})\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 3) (113, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Biomarker_1</th>\n",
       "      <th>Biomarker_2</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.262372</td>\n",
       "      <td>0.875473</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.336479</td>\n",
       "      <td>1.098616</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.182330</td>\n",
       "      <td>-1.609488</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.223131</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.587792</td>\n",
       "      <td>1.458617</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Biomarker_1  Biomarker_2  Diagnosis\n",
       "0      0.262372     0.875473        1.0\n",
       "5      0.336479     1.098616        1.0\n",
       "9      0.182330    -1.609488        2.0\n",
       "12    -0.223131     0.788462        1.0\n",
       "13     0.587792     1.458617        1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_train.shape, data_test.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Biomarker_1</th>\n",
       "      <th>Biomarker_2</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.344213</td>\n",
       "      <td>0.136095</td>\n",
       "      <td>1.392157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.830087</td>\n",
       "      <td>2.201512</td>\n",
       "      <td>0.677164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.302485</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.105349</td>\n",
       "      <td>-1.076236</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.222351</td>\n",
       "      <td>0.641859</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.470010</td>\n",
       "      <td>1.273894</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.032469</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Biomarker_1  Biomarker_2   Diagnosis\n",
       "count   102.000000   102.000000  102.000000\n",
       "mean      0.344213     0.136095    1.392157\n",
       "std       0.830087     2.201512    0.677164\n",
       "min      -2.302485   -11.512925    1.000000\n",
       "25%      -0.105349    -1.076236    1.000000\n",
       "50%       0.222351     0.641859    1.000000\n",
       "75%       0.470010     1.273894    2.000000\n",
       "max       4.032469     3.970292    3.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Fit Classification Models\n",
    "\n",
    "1. Generate a 2D scatter plot of the training set, denoting each class with a different color. Does it appear that the data points can be separated well by a linear classifier?\n",
    "\n",
    "2. Briefly explain the difference between multinomial logistic regression and one-vs-rest (OvR) logistic regression methods for fitting a multiclass classifier (in 2-3 sentences).\n",
    "\n",
    "3. Fit linear classification models on the thyroid data set using both the methods.  You should use $L_2$ regularization in both cases, tuning the regularization parameter using cross-validation.  Is there a difference in the overall classification accuracy of the two methods on the training and test sets?\n",
    "\n",
    "4. Also, compare the training and test accuracies of these models with the following classification methods:\n",
    "    - Multiclass Logistic Regression with quadratic terms \n",
    "    - Linear Discriminant Analysis\n",
    "    - Quadratic Discriminant Analysis\n",
    "    - k-Nearest Neighbors\n",
    "    <br>\n",
    "*Note:* you may use either the OvR or multinomial variant for the multiclass logistic regression (with $L_2$ regularization). Do not forget to use cross-validation to choose the regularization parameter, and also the number of neighbors in k-NN. \n",
    "\n",
    "5. Does the inclusion of the polynomial terms in logistic regression yield better test accuracy compared to the model with only linear terms? \n",
    "\n",
    "\n",
    "*Hint:* You may use the `KNeighborsClassifier` class to fit a k-NN classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGECAYAAACGdAwQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREREMlqEVEREQ01F/oYAVnOxTgKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e455eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train_1 = data_train.loc[data_train['Diagnosis'] == 1.0]\n",
    "data_train_2 = data_train.loc[data_train['Diagnosis'] == 2.0]\n",
    "data_train_3 = data_train.loc[data_train['Diagnosis'] == 3.0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.scatter(data_train_1.Biomarker_1, data_train_1.Biomarker_2, label=\"normal\", color=\"green\", alpha=0.3)\n",
    "ax.scatter(data_train_2.Biomarker_1, data_train_2.Biomarker_2, label=\"hyperthyroidism\", color=\"red\", alpha=0.3)\n",
    "ax.scatter(data_train_3.Biomarker_1, data_train_3.Biomarker_2, label=\"hypothyroidism\", color=\"blue\", alpha=0.3)\n",
    "ax.set_xlabel(\"Biomarker_1\")\n",
    "ax.set_ylabel(\"Biomarker_2\")\n",
    "ax.set_title(\"Scatter Plot of Diagnosis vs. Biomarkers\")\n",
    "ax.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 1. Generate a 2D scatter plot of the training set, denoting each class with a different color. Does it appear that the data points can be separated well by a linear classifier?\n",
    "- Possibly yes. We can draw a nealy horizontal line to divide green and red dots, though this may cause some green dots classified into red ones. We can also draw a line to divide blue and green dots, with only a few blue dots misclassified into green.\n",
    "- Generally speaking, the data points can be separated well by a linear clasifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Briefly explain the difference between multinomial logistic regression and one-vs-rest (OvR) logistic regression methods for fitting a multiclass classifier (in 2-3 sentences).\n",
    "- In multinomial logistic regression, dependent variable is nominal with more than two levels, such as \"normal\", \"hyperthyroidism\" and \"hypothyroidism\" in the above example. \n",
    "- One-vs-rest (OvR) compares one group to everything else, for example, normal and abnormal, where abnormal is hyperthyroidism and hypothyroidism. But there could be difference between hyperthyroidism and hypothyroidism, which can be analyzed using multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[[\"Biomarker_1\", \"Biomarker_2\"]]\n",
    "y_train = data_train[\"Diagnosis\"]\n",
    "X_test = data_test[[\"Biomarker_1\", \"Biomarker_2\"]]\n",
    "y_test = data_test[\"Diagnosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression classifier on train set has accuracy: 0.892156862745\n",
      "Multinomial Logistic regression classifier on test set has accuracy: 0.884955752212\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Logistic Regression\n",
    "X_train_mul = sm.add_constant(X_train)\n",
    "X_test_mul = sm.add_constant(X_test)\n",
    "reg_Cs = np.hstack((10.**np.arange(-5, 0), 10**np.arange(0, 6)))\n",
    "log_multi = LogisticRegressionCV(reg_Cs, cv=5, penalty='l2', multi_class=\"multinomial\")\n",
    "log_multi.fit(X_train_mul, y_train)\n",
    "print(\"Multinomial Logistic regression classifier on train set has accuracy:\", log_multi.score(X_train_mul, y_train))\n",
    "print(\"Multinomial Logistic regression classifier on test set has accuracy:\", log_multi.score(X_test_mul, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-vs-rest (OvR) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVR Logistic regression classifier on train set has accuracy: 0.843137254902\n",
      "OVR Logistic regression classifier on test set has accuracy: 0.840707964602\n"
     ]
    }
   ],
   "source": [
    "# One-vs-rest (OvR) Logistic Regression\n",
    "log_ovr = LogisticRegressionCV(reg_Cs, cv=5, penalty='l2', multi_class=\"ovr\")\n",
    "log_ovr.fit(X_train_mul, y_train)\n",
    "print(\"OVR Logistic regression classifier on train set has accuracy:\", log_ovr.score(X_train_mul, y_train))\n",
    "print(\"OVR Logistic regression classifier on test set has accuracy:\", log_ovr.score(X_test_mul, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit linear classification models on the thyroid data set using both the methods. You should use  $L_2$ regularization in both cases, tuning the regularization parameter using cross-validation. Is there a difference in the overall classification accuracy of the two methods on the training and test sets?\n",
    "- One obvious rsult is that accuracy score on train set is always higher than accuracy score on test set.\n",
    "- Overall classification accuracy of the multinomial method is higher than the one of the OvR method. I think this is because for the current dataset, the three classes are exclusive, and therefore multinomial logistic regression is a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Logistic Regression with quadratic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression with queadratic terms on train set has accuracy: 0.892156862745\n",
      "Multinomial Logistic regression with queadratic terms on test set has accuracy: 0.884955752212\n"
     ]
    }
   ],
   "source": [
    "# Multiclass Logistic Regression with quadratic terms\n",
    "X_train2 = X_train.copy()\n",
    "X_test2 = X_test.copy()\n",
    "X_train2[\"B1_sq\"] = X_train2[\"Biomarker_1\"]**2\n",
    "X_train2[\"B2_sq\"] = X_train2[\"Biomarker_2\"]**2\n",
    "X_test2[\"B1_sq\"] = X_test2[\"Biomarker_1\"]**2\n",
    "X_test2[\"B2_sq\"] = X_test2[\"Biomarker_2\"]**2\n",
    "\n",
    "X_train_mul2 = sm.add_constant(X_train2)\n",
    "X_test_mul2 = sm.add_constant(X_test2)\n",
    "log_multi2 = LogisticRegressionCV(reg_Cs, cv=5, penalty='l2', multi_class=\"multinomial\")\n",
    "log_multi2.fit(X_train_mul2, y_train)\n",
    "print(\"Multinomial Logistic regression with queadratic terms on train set has accuracy:\",\\\n",
    "      log_multi2.score(X_train_mul2, y_train))\n",
    "print(\"Multinomial Logistic regression with queadratic terms on test set has accuracy:\",\\\n",
    "      log_multi2.score(X_test_mul2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA on train set has accuracy: 0.872549019608\n",
      "LDA on test set has accuracy: 0.872549019608\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "# cross validation\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "model_LDA = LDA.fit(X_train, y_train)\n",
    "print(\"LDA on train set has accuracy:\", model_LDA.score(X_train, y_train))\n",
    "print(\"LDA on test set has accuracy:\", model_LDA.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86419753086419748, 0.86419753086419748, 0.87804878048780488, 0.86585365853658536, 0.87804878048780488]\n",
      "[0.8571428571428571, 0.90476190476190477, 0.84999999999999998, 0.90000000000000002, 0.94999999999999996]\n",
      "0.870069256248 0.892380952381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yijunshen/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "\n",
    "for itrain, ivalid in KFold(n_splits=5, shuffle=True, random_state=9001).split(X_train.index):\n",
    "    # in general though its good for creating consistent psets, don't put seeds into kfold\n",
    "    # split\n",
    "    X_train_cv = X_train.iloc[itrain,:]\n",
    "    y_train_cv = y_train.iloc[itrain]\n",
    "    X_valid_cv = X_train.iloc[ivalid,:]\n",
    "    y_valid_cv = y_train.iloc[ivalid]\n",
    "    \n",
    "    lda.fit(X_train_cv, y_train_cv)\n",
    "    train_scores.append(lda.score(X_train_cv, y_train_cv))\n",
    "    valid_scores.append(lda.score(X_valid_cv, y_valid_cv))\n",
    "\n",
    "train_scores_mean = np.mean(train_scores)\n",
    "valid_scores_mean = np.mean(valid_scores)\n",
    "print(train_scores)\n",
    "print(valid_scores)\n",
    "print(train_scores_mean, valid_scores_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA on train set has accuracy: 0.872549019608\n",
      "QDA on test set has accuracy: 0.872549019608\n"
     ]
    }
   ],
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "# cross validation\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "model_QDA = QDA.fit(X_train, y_train)\n",
    "print(\"QDA on train set has accuracy:\", model_QDA.score(X_train, y_train))\n",
    "print(\"QDA on test set has accuracy:\", model_QDA.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 4. Compare the training and test accuracies of these models with these classification methods:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 5. Does the inclusion of the polynomial terms in logistic regression yield better test accuracy compared to the model with only linear terms?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Visualize Decision Boundaries\n",
    "\n",
    "The following code will allow you to visualize the decision boundaries of a given classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  plot_decision_boundary\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      x (predictors)\n",
    "#      y (labels)\n",
    "#      model (the classifier you want to visualize)\n",
    "#      title (title for plot)\n",
    "#      ax (a set of axes to plot on)\n",
    "#      poly_degree (highest degree of polynomial terms included in the model; None by default)\n",
    "\n",
    "def plot_decision_boundary(x, y, model, title, ax, poly_degree=None):\n",
    "    # Create mesh\n",
    "    # Interval of points for biomarker 1\n",
    "    min0 = x[:,0].min()\n",
    "    max0 = x[:,0].max()\n",
    "    interval0 = np.arange(min0, max0, (max0-min0)/100)\n",
    "    n0 = np.size(interval0)\n",
    "    \n",
    "    # Interval of points for biomarker 2\n",
    "    min1 = x[:,1].min()\n",
    "    max1 = x[:,1].max()\n",
    "    interval1 = np.arange(min1, max1, (max1-min1)/100)\n",
    "    n1 = np.size(interval1)\n",
    "\n",
    "    # Create mesh grid of points\n",
    "    x1, x2 = np.meshgrid(interval0, interval1)\n",
    "    x1 = x1.reshape(-1,1)\n",
    "    x2 = x2.reshape(-1,1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # Predict on mesh of points\n",
    "    # Check if polynomial terms need to be included\n",
    "    if(poly_degree!=None):\n",
    "        # Use PolynomialFeatures to generate polynomial terms\n",
    "        poly = PolynomialFeatures(poly_degree)\n",
    "        xx_ = poly.fit_transform(xx)\n",
    "        yy = model.predict(xx_) \n",
    "    else:   \n",
    "        yy = model.predict(xx)\n",
    "        \n",
    "    yy = yy.reshape((n0, n1))\n",
    "\n",
    "    # Plot decision surface\n",
    "    x1 = x1.reshape(n0, n1)\n",
    "    x2 = x2.reshape(n0, n1)\n",
    "    ax.contourf(x1, x2, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    \n",
    "    # Plot scatter plot of data\n",
    "    yy = y.reshape(-1,)\n",
    "    ax.scatter(x[yy==1,0], x[yy==1,1], c='blue', label='Normal', cmap=plt.cm.coolwarm)\n",
    "    ax.scatter(x[yy==2,0], x[yy==2,1], c='cyan', label='Hyper', cmap=plt.cm.coolwarm)\n",
    "    ax.scatter(x[yy==3,0], x[yy==3,1], c='red', label='Hypo', cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Label axis, title\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Biomarker 1')\n",
    "    ax.set_ylabel('Biomarker 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The provided code uses `sklearn`'s `PolynomialFeatures` to generate higher-order polynomial terms, with degree `poly_degree`. \n",
    "Also, if you have loaded the data sets into `pandas` data frames, you may use the `as_matrix` function to obtain a `numpy` array from the data frame objects.\n",
    "\n",
    "1. Use the above code to visualize the decision boundaries for each of the model fitted in the previous part.\n",
    "2. Comment on the difference in the decision boundaries (if any) for the OvR and multinomial logistic regression models. Is there a difference between the decision boundaries for the linear logistic regression models and LDA. What about the decision boundaries for the quadratic logistic regression and QDA? Give an explanation for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Fit Decision Trees\n",
    "\n",
    "We next try out decision trees for thyroid classification. For the following questions, you may use the *Gini* index as the splitting criterion while fitting the decision tree. \n",
    "1. Fit a decision tree model to the thyroid data set with (maximum) tree depths 2, 3, ..., 10. Make plots of the training and test accuracies as a function of the tree depth. Is there a depth at which the fitted decision tree model achieves near-perfect classification on the training set? If so, what can you say about the test accuracy of this model?\n",
    "\n",
    "2. Use 5-fold cross-validation to find the optimal tree depth. How does the performance of a decision tree fitted with this depth compare with the models fitted in Part 2(a)?\n",
    "\n",
    "3. Use the code provided in Part 2(c) to visualize the decision boundary of the fitted decision tree. How is the decision boundary of the decision tree model different from the other methods? Given an explanation for your observation.\n",
    "\n",
    "4. Use the `export_graphviz` function in `sklearn` to generate a visualization of the tree diagram for the fitted model. Based on the visualization, explain *in words* how the fitted model diagnoses 'hypothyroidism' for a patient.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Look at the `export_graphviz` function in the `sklearn.tree` module. \n",
    "\n",
    "You can get a graphic for this visualization by pasting the generated graphviz file in the text box at http://www.webgraphviz.com/ , or you can do it on your own computer.\n",
    "\n",
    "If you choose the do the latter, you will have to install `GraphViz` and `pydot` to use the decision tree rendering code. For this, you may execute the following commands in a terminal:\n",
    "\n",
    "`$pip install graphviz`<br>\n",
    "`$pip install pydot`\n",
    "\n",
    "*Hint:* You may use the `DecisionTreeClassifier` class to fit a decision tree classifier and the `max_depth` attribute to set the tree depth. You may use the `cross_val_score` function for cross-validation with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Too many models to choose from!\n",
    "\n",
    "We have so far seen six different ways of fitting a classification model for thyroid classification problem: **linear logistic regression**, **logistic regression with polynomial terms**, **LDA**, **QDA**, **k-NN** and **decision tree**. Which of these methods should one use in practice? To answer this question, we now look at the pros and cons of each method.\n",
    "\n",
    "1. Compare and contrast the six models based on each of the following criteria (a supporting table to summarize your  thoughts can be helpful):\n",
    "  - Classification performance\n",
    "  - Complexity of decision boundary\n",
    "  - Memory storage\n",
    "  - Ease of interpretability\n",
    "\n",
    "2. If you were a clinician who had to use the classifier to diagnose thyroid disorders in patients, which among the six methods would you be most comfortable in using? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Including an 'abstain' option\n",
    "\n",
    "One of the reasons a hospital might be hesitant to use your thyroid classification model is that a misdiagnosis by the model on a patient can sometimes prove to be very costly (e.g. if the patient were to file a law suit seeking a compensation for damages). One way to mitigate this concern is to allow the model to 'abstain' from making a prediction, whenever it is uncertain about the diagnosis for a patient. However, when the model abstains from making a prediction, the hospital will have to forward the patient to a thyroid specialist (i.e. an endocrinologist), which would incur additional cost. How does one design a thyroid classification model with an abstain option, such that the cost to the hospital is minimized?\n",
    "\n",
    "1. More specifically, suppose the cost incurred by a hospital when a model mis-predicts on a patient is \\$5000, and the cost incurred when the model abstains from making a prediction is \\$1000. What is the average cost per patient for the OvR logistic regression model from Question 1, Part 3? Note that this needs to be evaluated on the patients in the test set. Your task is to design a classification strategy (into the 3 groups plus the *abstain* group) that has as low cost as possible per patient.   Give a justification for your approach.\n",
    "\n",
    "2. **Presentation:** Prepare a set of 5 slides explaining your approach to the hospital management. Your presentation must be accessible to the lay man. Explain in particular how your approach would be robust to changes in the costs of using the abstain option.\n",
    "\n",
    "*Hint:* think of a way to use the estimated probabilities from the logistic regression model to decide who to classify as *abstain*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
