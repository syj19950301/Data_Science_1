{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
    "# Reg-Logistic Regression, ROC, and Data Imputation\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yijunshen/anaconda3/lib/python3.6/site-packages/seaborn/apionly.py:6: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn.apionly as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Breast Cancer Detection\n",
    "\n",
    "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
    "\n",
    "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Beyond Classification Accuracy\n",
    "\n",
    "\n",
    "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
    "\n",
    "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
    "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
    "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "    \n",
    "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
    "\n",
    "<ol start=\"3\">\n",
    "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
    "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
    "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
    "</ol>\n",
    "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.439999999999999891e-01</th>\n",
       "      <th>-1.429999999999999882e-01</th>\n",
       "      <th>-1.160000000000000059e-01</th>\n",
       "      <th>-1.029999999999999943e-01</th>\n",
       "      <th>2.260000000000000064e-01</th>\n",
       "      <th>2.099999999999999922e-01</th>\n",
       "      <th>-9.799999999999999822e-01</th>\n",
       "      <th>-7.800000000000000266e-01</th>\n",
       "      <th>-4.739999999999999769e-01</th>\n",
       "      <th>-4.470000000000000084e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>9.250000000000000444e-01</th>\n",
       "      <th>5.160000000000000142e-01</th>\n",
       "      <th>3.439999999999999725e-01</th>\n",
       "      <th>9.060000000000000275e-01</th>\n",
       "      <th>-1.129999999999999893e+00</th>\n",
       "      <th>-5.520000000000000462e-01</th>\n",
       "      <th>5.530000000000000471e-01</th>\n",
       "      <th>-4.169999999999999818e-01</th>\n",
       "      <th>2.560000000000000053e-01</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.01100</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>-0.1730</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.0522</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.00785</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.0723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21200</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>-1.8100</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.39400</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-1.1800</td>\n",
       "      <td>-2.700</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-2.650</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21500</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.9930</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.0528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.27900</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-1.3200</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.29500</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00922</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-1.6900</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1.439999999999999891e-01  -1.429999999999999882e-01  \\\n",
       "0                   -0.01100                      0.138   \n",
       "1                    0.21200                     -0.313   \n",
       "2                    0.21500                     -0.184   \n",
       "3                    0.27900                     -0.197   \n",
       "4                    0.00922                     -0.138   \n",
       "\n",
       "   -1.160000000000000059e-01  -1.029999999999999943e-01  \\\n",
       "0                    -0.2230                    -0.1730   \n",
       "1                     0.2660                     0.2320   \n",
       "2                     0.0274                     0.0494   \n",
       "3                     0.1270                     0.0973   \n",
       "4                     0.1690                     0.1540   \n",
       "\n",
       "   2.260000000000000064e-01  2.099999999999999922e-01  \\\n",
       "0                     0.188                     0.284   \n",
       "1                    -1.190                    -1.150   \n",
       "2                     0.443                     0.463   \n",
       "3                    -0.213                    -0.150   \n",
       "4                    -0.391                    -0.397   \n",
       "\n",
       "   -9.799999999999999822e-01  -7.800000000000000266e-01  \\\n",
       "0                    -0.0522                     -0.256   \n",
       "1                    -1.8100                     -1.560   \n",
       "2                    -1.0500                     -0.941   \n",
       "3                    -1.3200                     -0.994   \n",
       "4                    -1.6900                     -1.450   \n",
       "\n",
       "   -4.739999999999999769e-01  -4.470000000000000084e-01  \\\n",
       "0                      0.129                      0.427   \n",
       "1                     -1.250                     -1.200   \n",
       "2                     -0.531                     -0.394   \n",
       "3                     -1.110                     -1.090   \n",
       "4                     -0.546                     -0.527   \n",
       "\n",
       "             ...             9.250000000000000444e-01  \\\n",
       "0            ...                               -0.593   \n",
       "1            ...                               -0.816   \n",
       "2            ...                                0.634   \n",
       "3            ...                               -0.640   \n",
       "4            ...                               -0.277   \n",
       "\n",
       "   5.160000000000000142e-01  3.439999999999999725e-01  \\\n",
       "0                     0.452                   0.00785   \n",
       "1                     1.570                   0.39400   \n",
       "2                     0.111                   0.37100   \n",
       "3                     0.485                   0.29500   \n",
       "4                     0.699                   0.37100   \n",
       "\n",
       "   9.060000000000000275e-01  -1.129999999999999893e+00  \\\n",
       "0                    -0.533                    -0.0789   \n",
       "1                     1.340                    -1.1800   \n",
       "2                     0.859                    -0.9930   \n",
       "3                     0.403                    -1.1200   \n",
       "4                     0.481                    -1.0600   \n",
       "\n",
       "   -5.520000000000000462e-01  5.530000000000000471e-01  \\\n",
       "0                      0.705                     0.906   \n",
       "1                     -2.700                    -0.926   \n",
       "2                     -0.492                     0.363   \n",
       "3                     -0.343                     0.468   \n",
       "4                     -0.526                     0.550   \n",
       "\n",
       "   -4.169999999999999818e-01  2.560000000000000053e-01  \\\n",
       "0                      0.216                   -0.0723   \n",
       "1                     -2.650                   -0.0447   \n",
       "2                      0.326                   -0.0528   \n",
       "3                     -0.820                    0.4350   \n",
       "4                     -0.284                    0.1550   \n",
       "\n",
       "   0.000000000000000000e+00  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(9001)\n",
    "df = pd.read_csv('hw6_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.439999999999999891e-01</th>\n",
       "      <th>-1.429999999999999882e-01</th>\n",
       "      <th>-1.160000000000000059e-01</th>\n",
       "      <th>-1.029999999999999943e-01</th>\n",
       "      <th>2.260000000000000064e-01</th>\n",
       "      <th>2.099999999999999922e-01</th>\n",
       "      <th>-9.799999999999999822e-01</th>\n",
       "      <th>-7.800000000000000266e-01</th>\n",
       "      <th>-4.739999999999999769e-01</th>\n",
       "      <th>-4.470000000000000084e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>9.250000000000000444e-01</th>\n",
       "      <th>5.160000000000000142e-01</th>\n",
       "      <th>3.439999999999999725e-01</th>\n",
       "      <th>9.060000000000000275e-01</th>\n",
       "      <th>-1.129999999999999893e+00</th>\n",
       "      <th>-5.520000000000000462e-01</th>\n",
       "      <th>5.530000000000000471e-01</th>\n",
       "      <th>-4.169999999999999818e-01</th>\n",
       "      <th>2.560000000000000053e-01</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "      <td>69097.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000998</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.003164</td>\n",
       "      <td>-0.002645</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>-0.001249</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>-0.000894</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.001287</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.005919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.211814</td>\n",
       "      <td>1.057315</td>\n",
       "      <td>1.025786</td>\n",
       "      <td>1.033030</td>\n",
       "      <td>0.983603</td>\n",
       "      <td>0.988942</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>1.005921</td>\n",
       "      <td>1.004499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003115</td>\n",
       "      <td>0.999683</td>\n",
       "      <td>0.998781</td>\n",
       "      <td>1.003225</td>\n",
       "      <td>1.000271</td>\n",
       "      <td>0.999411</td>\n",
       "      <td>1.000923</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>1.024495</td>\n",
       "      <td>0.076709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-316.000000</td>\n",
       "      <td>-147.000000</td>\n",
       "      <td>-71.400000</td>\n",
       "      <td>-81.400000</td>\n",
       "      <td>-3.080000</td>\n",
       "      <td>-9.120000</td>\n",
       "      <td>-2.070000</td>\n",
       "      <td>-1.910000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.200000</td>\n",
       "      <td>-7.460000</td>\n",
       "      <td>-32.900000</td>\n",
       "      <td>-7.780000</td>\n",
       "      <td>-1.220000</td>\n",
       "      <td>-4.710000</td>\n",
       "      <td>-5.450000</td>\n",
       "      <td>-6.340000</td>\n",
       "      <td>-23.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.064700</td>\n",
       "      <td>-0.040900</td>\n",
       "      <td>-0.253000</td>\n",
       "      <td>-0.214000</td>\n",
       "      <td>-0.569000</td>\n",
       "      <td>-0.553000</td>\n",
       "      <td>-0.532000</td>\n",
       "      <td>-0.597000</td>\n",
       "      <td>-0.711000</td>\n",
       "      <td>-0.728000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458000</td>\n",
       "      <td>-0.491000</td>\n",
       "      <td>-0.334000</td>\n",
       "      <td>-0.520000</td>\n",
       "      <td>-0.857000</td>\n",
       "      <td>-0.247000</td>\n",
       "      <td>-0.606000</td>\n",
       "      <td>-0.097800</td>\n",
       "      <td>-0.259000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>-0.099200</td>\n",
       "      <td>-0.098800</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>-0.075300</td>\n",
       "      <td>-0.227000</td>\n",
       "      <td>-0.235000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102000</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>-0.269000</td>\n",
       "      <td>-0.132000</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>-0.142000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.340000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.310000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       -1.439999999999999891e-01  -1.429999999999999882e-01  \\\n",
       "count               69097.000000               69097.000000   \n",
       "mean                   -0.000998                  -0.001317   \n",
       "std                     1.211814                   1.057315   \n",
       "min                  -316.000000                -147.000000   \n",
       "25%                    -0.064700                  -0.040900   \n",
       "50%                     0.024400                   0.103000   \n",
       "75%                     0.094200                   0.176000   \n",
       "max                     1.340000                   3.750000   \n",
       "\n",
       "       -1.160000000000000059e-01  -1.029999999999999943e-01  \\\n",
       "count               69097.000000               69097.000000   \n",
       "mean                    0.000783                   0.000315   \n",
       "std                     1.025786                   1.033030   \n",
       "min                   -71.400000                 -81.400000   \n",
       "25%                    -0.253000                  -0.214000   \n",
       "50%                     0.095600                   0.085000   \n",
       "75%                     0.361000                   0.313000   \n",
       "max                     2.030000                   1.750000   \n",
       "\n",
       "       2.260000000000000064e-01  2.099999999999999922e-01  \\\n",
       "count              69097.000000              69097.000000   \n",
       "mean                  -0.003164                 -0.002645   \n",
       "std                    0.983603                  0.988942   \n",
       "min                   -3.080000                 -9.120000   \n",
       "25%                   -0.569000                 -0.553000   \n",
       "50%                   -0.099200                 -0.098800   \n",
       "75%                    0.450000                  0.434000   \n",
       "max                   28.000000                 26.900000   \n",
       "\n",
       "       -9.799999999999999822e-01  -7.800000000000000266e-01  \\\n",
       "count               69097.000000               69097.000000   \n",
       "mean                    0.000860                  -0.000371   \n",
       "std                     0.999775                   0.998938   \n",
       "min                    -2.070000                  -1.910000   \n",
       "25%                    -0.532000                  -0.597000   \n",
       "50%                     0.013700                  -0.075300   \n",
       "75%                     0.582000                   0.513000   \n",
       "max                    11.200000                  22.200000   \n",
       "\n",
       "       -4.739999999999999769e-01  -4.470000000000000084e-01  \\\n",
       "count               69097.000000               69097.000000   \n",
       "mean                    0.002183                   0.002589   \n",
       "std                     1.005921                   1.004499   \n",
       "min                    -1.570000                  -1.500000   \n",
       "25%                    -0.711000                  -0.728000   \n",
       "50%                    -0.227000                  -0.235000   \n",
       "75%                     0.488000                   0.498000   \n",
       "max                    16.700000                  22.500000   \n",
       "\n",
       "                 ...             9.250000000000000444e-01  \\\n",
       "count            ...                         69097.000000   \n",
       "mean             ...                             0.002552   \n",
       "std              ...                             1.003115   \n",
       "min              ...                           -15.200000   \n",
       "25%              ...                            -0.458000   \n",
       "50%              ...                            -0.102000   \n",
       "75%              ...                             0.322000   \n",
       "max              ...                            13.000000   \n",
       "\n",
       "       5.160000000000000142e-01  3.439999999999999725e-01  \\\n",
       "count              69097.000000              69097.000000   \n",
       "mean                  -0.001249                 -0.001666   \n",
       "std                    0.999683                  0.998781   \n",
       "min                   -7.460000                -32.900000   \n",
       "25%                   -0.491000                 -0.334000   \n",
       "50%                    0.161000                  0.205000   \n",
       "75%                    0.559000                  0.399000   \n",
       "max                    6.310000                 17.000000   \n",
       "\n",
       "       9.060000000000000275e-01  -1.129999999999999893e+00  \\\n",
       "count              69097.000000               69097.000000   \n",
       "mean                   0.002447                  -0.000894   \n",
       "std                    1.003225                   1.000271   \n",
       "min                   -7.780000                  -1.220000   \n",
       "25%                   -0.520000                  -0.857000   \n",
       "50%                   -0.269000                  -0.132000   \n",
       "75%                    0.134000                   0.645000   \n",
       "max                   10.600000                  17.700000   \n",
       "\n",
       "       -5.520000000000000462e-01  5.530000000000000471e-01  \\\n",
       "count               69097.000000              69097.000000   \n",
       "mean                    0.000008                 -0.001287   \n",
       "std                     0.999411                  1.000923   \n",
       "min                    -4.710000                 -5.450000   \n",
       "25%                    -0.247000                 -0.606000   \n",
       "50%                     0.321000                  0.269000   \n",
       "75%                     0.645000                  0.801000   \n",
       "max                     1.410000                  1.440000   \n",
       "\n",
       "       -4.169999999999999818e-01  2.560000000000000053e-01  \\\n",
       "count               69097.000000              69097.000000   \n",
       "mean                    0.000274                  0.000840   \n",
       "std                     0.999324                  1.024495   \n",
       "min                    -6.340000                -23.200000   \n",
       "25%                    -0.097800                 -0.259000   \n",
       "50%                     0.326000                 -0.142000   \n",
       "75%                     0.583000                  0.006700   \n",
       "max                     1.420000                 40.600000   \n",
       "\n",
       "       0.000000000000000000e+00  \n",
       "count              69097.000000  \n",
       "mean                   0.005919  \n",
       "std                    0.076709  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 118 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51822, 117), (51822,), (17275, 117), (17275,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test = (X_test - X_train.mean()) / X_train.std()\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.439999999999999891e-01</th>\n",
       "      <th>-1.429999999999999882e-01</th>\n",
       "      <th>-1.160000000000000059e-01</th>\n",
       "      <th>-1.029999999999999943e-01</th>\n",
       "      <th>2.260000000000000064e-01</th>\n",
       "      <th>2.099999999999999922e-01</th>\n",
       "      <th>-9.799999999999999822e-01</th>\n",
       "      <th>-7.800000000000000266e-01</th>\n",
       "      <th>-4.739999999999999769e-01</th>\n",
       "      <th>-4.470000000000000084e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>-7.379999999999999893e-01</th>\n",
       "      <th>9.250000000000000444e-01</th>\n",
       "      <th>5.160000000000000142e-01</th>\n",
       "      <th>3.439999999999999725e-01</th>\n",
       "      <th>9.060000000000000275e-01</th>\n",
       "      <th>-1.129999999999999893e+00</th>\n",
       "      <th>-5.520000000000000462e-01</th>\n",
       "      <th>5.530000000000000471e-01</th>\n",
       "      <th>-4.169999999999999818e-01</th>\n",
       "      <th>2.560000000000000053e-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "      <td>5.182200e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.784788e-16</td>\n",
       "      <td>2.960231e-17</td>\n",
       "      <td>-2.973620e-18</td>\n",
       "      <td>-9.424320e-18</td>\n",
       "      <td>6.062501e-17</td>\n",
       "      <td>5.598355e-17</td>\n",
       "      <td>2.056254e-17</td>\n",
       "      <td>-1.965846e-17</td>\n",
       "      <td>-1.369194e-17</td>\n",
       "      <td>-2.375897e-18</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.780626e-18</td>\n",
       "      <td>-2.451523e-17</td>\n",
       "      <td>-4.105867e-18</td>\n",
       "      <td>-7.341929e-18</td>\n",
       "      <td>-2.129309e-17</td>\n",
       "      <td>5.221832e-17</td>\n",
       "      <td>4.601908e-17</td>\n",
       "      <td>5.343733e-17</td>\n",
       "      <td>4.478641e-17</td>\n",
       "      <td>2.621268e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.262472e+02</td>\n",
       "      <td>-1.439716e+02</td>\n",
       "      <td>-6.890080e+01</td>\n",
       "      <td>-7.938560e+01</td>\n",
       "      <td>-3.115148e+00</td>\n",
       "      <td>-9.153742e+00</td>\n",
       "      <td>-2.067937e+00</td>\n",
       "      <td>-1.908246e+00</td>\n",
       "      <td>-1.559753e+00</td>\n",
       "      <td>-1.493721e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.939020e+00</td>\n",
       "      <td>-1.500584e+01</td>\n",
       "      <td>-6.417456e+00</td>\n",
       "      <td>-1.274658e+01</td>\n",
       "      <td>-6.573638e+00</td>\n",
       "      <td>-1.216458e+00</td>\n",
       "      <td>-4.696535e+00</td>\n",
       "      <td>-5.446999e+00</td>\n",
       "      <td>-6.336429e+00</td>\n",
       "      <td>-2.325851e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.436865e-02</td>\n",
       "      <td>-3.899734e-02</td>\n",
       "      <td>-2.430020e-01</td>\n",
       "      <td>-2.082215e-01</td>\n",
       "      <td>-5.725497e-01</td>\n",
       "      <td>-5.517512e-01</td>\n",
       "      <td>-5.326718e-01</td>\n",
       "      <td>-5.962096e-01</td>\n",
       "      <td>-7.059286e-01</td>\n",
       "      <td>-7.267595e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.410940e-01</td>\n",
       "      <td>-4.555016e-01</td>\n",
       "      <td>-4.881047e-01</td>\n",
       "      <td>-3.396451e-01</td>\n",
       "      <td>-5.203783e-01</td>\n",
       "      <td>-8.552714e-01</td>\n",
       "      <td>-2.425729e-01</td>\n",
       "      <td>-6.056973e-01</td>\n",
       "      <td>-9.863111e-02</td>\n",
       "      <td>-2.599988e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.942509e-02</td>\n",
       "      <td>1.022342e-01</td>\n",
       "      <td>9.165048e-02</td>\n",
       "      <td>8.249608e-02</td>\n",
       "      <td>-9.644555e-02</td>\n",
       "      <td>-9.790508e-02</td>\n",
       "      <td>1.331122e-02</td>\n",
       "      <td>-7.549198e-02</td>\n",
       "      <td>-2.277771e-01</td>\n",
       "      <td>-2.383312e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.167339e-02</td>\n",
       "      <td>-1.041777e-01</td>\n",
       "      <td>1.597152e-01</td>\n",
       "      <td>2.086718e-01</td>\n",
       "      <td>-2.700964e-01</td>\n",
       "      <td>-1.309030e-01</td>\n",
       "      <td>3.231522e-01</td>\n",
       "      <td>2.680619e-01</td>\n",
       "      <td>3.262189e-01</td>\n",
       "      <td>-1.416979e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.961522e-02</td>\n",
       "      <td>1.737315e-01</td>\n",
       "      <td>3.456312e-01</td>\n",
       "      <td>3.037769e-01</td>\n",
       "      <td>4.596846e-01</td>\n",
       "      <td>4.382760e-01</td>\n",
       "      <td>5.790719e-01</td>\n",
       "      <td>5.149744e-01</td>\n",
       "      <td>4.848597e-01</td>\n",
       "      <td>4.948085e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.559156e-01</td>\n",
       "      <td>3.132661e-01</td>\n",
       "      <td>5.596040e-01</td>\n",
       "      <td>4.039211e-01</td>\n",
       "      <td>1.305525e-01</td>\n",
       "      <td>6.443508e-01</td>\n",
       "      <td>6.464236e-01</td>\n",
       "      <td>8.020676e-01</td>\n",
       "      <td>5.841278e-01</td>\n",
       "      <td>7.246089e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.615820e-01</td>\n",
       "      <td>3.675138e+00</td>\n",
       "      <td>1.957135e+00</td>\n",
       "      <td>1.705190e+00</td>\n",
       "      <td>2.836850e+01</td>\n",
       "      <td>2.701338e+01</td>\n",
       "      <td>1.118709e+01</td>\n",
       "      <td>2.218406e+01</td>\n",
       "      <td>1.657378e+01</td>\n",
       "      <td>2.238057e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.268888e+01</td>\n",
       "      <td>1.282375e+01</td>\n",
       "      <td>6.309006e+00</td>\n",
       "      <td>1.719840e+01</td>\n",
       "      <td>1.048114e+01</td>\n",
       "      <td>1.586507e+01</td>\n",
       "      <td>1.399726e+00</td>\n",
       "      <td>1.443074e+00</td>\n",
       "      <td>1.350857e+00</td>\n",
       "      <td>3.749606e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       -1.439999999999999891e-01  -1.429999999999999882e-01  \\\n",
       "count               5.182200e+04               5.182200e+04   \n",
       "mean               -1.784788e-16               2.960231e-17   \n",
       "std                 1.000000e+00               1.000000e+00   \n",
       "min                -2.262472e+02              -1.439716e+02   \n",
       "25%                -4.436865e-02              -3.899734e-02   \n",
       "50%                 1.942509e-02               1.022342e-01   \n",
       "75%                 6.961522e-02               1.737315e-01   \n",
       "max                 9.615820e-01               3.675138e+00   \n",
       "\n",
       "       -1.160000000000000059e-01  -1.029999999999999943e-01  \\\n",
       "count               5.182200e+04               5.182200e+04   \n",
       "mean               -2.973620e-18              -9.424320e-18   \n",
       "std                 1.000000e+00               1.000000e+00   \n",
       "min                -6.890080e+01              -7.938560e+01   \n",
       "25%                -2.430020e-01              -2.082215e-01   \n",
       "50%                 9.165048e-02               8.249608e-02   \n",
       "75%                 3.456312e-01               3.037769e-01   \n",
       "max                 1.957135e+00               1.705190e+00   \n",
       "\n",
       "       2.260000000000000064e-01  2.099999999999999922e-01  \\\n",
       "count              5.182200e+04              5.182200e+04   \n",
       "mean               6.062501e-17              5.598355e-17   \n",
       "std                1.000000e+00              1.000000e+00   \n",
       "min               -3.115148e+00             -9.153742e+00   \n",
       "25%               -5.725497e-01             -5.517512e-01   \n",
       "50%               -9.644555e-02             -9.790508e-02   \n",
       "75%                4.596846e-01              4.382760e-01   \n",
       "max                2.836850e+01              2.701338e+01   \n",
       "\n",
       "       -9.799999999999999822e-01  -7.800000000000000266e-01  \\\n",
       "count               5.182200e+04               5.182200e+04   \n",
       "mean                2.056254e-17              -1.965846e-17   \n",
       "std                 1.000000e+00               1.000000e+00   \n",
       "min                -2.067937e+00              -1.908246e+00   \n",
       "25%                -5.326718e-01              -5.962096e-01   \n",
       "50%                 1.331122e-02              -7.549198e-02   \n",
       "75%                 5.790719e-01               5.149744e-01   \n",
       "max                 1.118709e+01               2.218406e+01   \n",
       "\n",
       "       -4.739999999999999769e-01  -4.470000000000000084e-01  \\\n",
       "count               5.182200e+04               5.182200e+04   \n",
       "mean               -1.369194e-17              -2.375897e-18   \n",
       "std                 1.000000e+00               1.000000e+00   \n",
       "min                -1.559753e+00              -1.493721e+00   \n",
       "25%                -7.059286e-01              -7.267595e-01   \n",
       "50%                -2.277771e-01              -2.383312e-01   \n",
       "75%                 4.848597e-01               4.948085e-01   \n",
       "max                 1.657378e+01               2.238057e+01   \n",
       "\n",
       "                 ...             -7.379999999999999893e-01  \\\n",
       "count            ...                          5.182200e+04   \n",
       "mean             ...                         -6.780626e-18   \n",
       "std              ...                          1.000000e+00   \n",
       "min              ...                         -1.939020e+00   \n",
       "25%              ...                         -6.410940e-01   \n",
       "50%              ...                         -6.167339e-02   \n",
       "75%              ...                          5.559156e-01   \n",
       "max              ...                          1.268888e+01   \n",
       "\n",
       "       9.250000000000000444e-01  5.160000000000000142e-01  \\\n",
       "count              5.182200e+04              5.182200e+04   \n",
       "mean              -2.451523e-17             -4.105867e-18   \n",
       "std                1.000000e+00              1.000000e+00   \n",
       "min               -1.500584e+01             -6.417456e+00   \n",
       "25%               -4.555016e-01             -4.881047e-01   \n",
       "50%               -1.041777e-01              1.597152e-01   \n",
       "75%                3.132661e-01              5.596040e-01   \n",
       "max                1.282375e+01              6.309006e+00   \n",
       "\n",
       "       3.439999999999999725e-01  9.060000000000000275e-01  \\\n",
       "count              5.182200e+04              5.182200e+04   \n",
       "mean              -7.341929e-18             -2.129309e-17   \n",
       "std                1.000000e+00              1.000000e+00   \n",
       "min               -1.274658e+01             -6.573638e+00   \n",
       "25%               -3.396451e-01             -5.203783e-01   \n",
       "50%                2.086718e-01             -2.700964e-01   \n",
       "75%                4.039211e-01              1.305525e-01   \n",
       "max                1.719840e+01              1.048114e+01   \n",
       "\n",
       "       -1.129999999999999893e+00  -5.520000000000000462e-01  \\\n",
       "count               5.182200e+04               5.182200e+04   \n",
       "mean                5.221832e-17               4.601908e-17   \n",
       "std                 1.000000e+00               1.000000e+00   \n",
       "min                -1.216458e+00              -4.696535e+00   \n",
       "25%                -8.552714e-01              -2.425729e-01   \n",
       "50%                -1.309030e-01               3.231522e-01   \n",
       "75%                 6.443508e-01               6.464236e-01   \n",
       "max                 1.586507e+01               1.399726e+00   \n",
       "\n",
       "       5.530000000000000471e-01  -4.169999999999999818e-01  \\\n",
       "count              5.182200e+04               5.182200e+04   \n",
       "mean               5.343733e-17               4.478641e-17   \n",
       "std                1.000000e+00               1.000000e+00   \n",
       "min               -5.446999e+00              -6.336429e+00   \n",
       "25%               -6.056973e-01              -9.863111e-02   \n",
       "50%                2.680619e-01               3.262189e-01   \n",
       "75%                8.020676e-01               5.841278e-01   \n",
       "max                1.443074e+00               1.350857e+00   \n",
       "\n",
       "       2.560000000000000053e-01  \n",
       "count              5.182200e+04  \n",
       "mean               2.621268e-16  \n",
       "std                1.000000e+00  \n",
       "min               -2.325851e+01  \n",
       "25%               -2.599988e-01  \n",
       "50%               -1.416979e-01  \n",
       "75%                7.246089e-03  \n",
       "max                3.749606e+01  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.439999999999999891e-01</th>\n",
       "      <th>-1.429999999999999882e-01</th>\n",
       "      <th>-1.160000000000000059e-01</th>\n",
       "      <th>-1.029999999999999943e-01</th>\n",
       "      <th>2.260000000000000064e-01</th>\n",
       "      <th>2.099999999999999922e-01</th>\n",
       "      <th>-9.799999999999999822e-01</th>\n",
       "      <th>-7.800000000000000266e-01</th>\n",
       "      <th>-4.739999999999999769e-01</th>\n",
       "      <th>-4.470000000000000084e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>-7.379999999999999893e-01</th>\n",
       "      <th>9.250000000000000444e-01</th>\n",
       "      <th>5.160000000000000142e-01</th>\n",
       "      <th>3.439999999999999725e-01</th>\n",
       "      <th>9.060000000000000275e-01</th>\n",
       "      <th>-1.129999999999999893e+00</th>\n",
       "      <th>-5.520000000000000462e-01</th>\n",
       "      <th>5.530000000000000471e-01</th>\n",
       "      <th>-4.169999999999999818e-01</th>\n",
       "      <th>2.560000000000000053e-01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "      <td>17275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>-0.002335</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>-0.006485</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>-0.005862</td>\n",
       "      <td>-0.005302</td>\n",
       "      <td>-0.001169</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.005348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.147718</td>\n",
       "      <td>1.159427</td>\n",
       "      <td>0.993605</td>\n",
       "      <td>1.055627</td>\n",
       "      <td>0.972808</td>\n",
       "      <td>0.967692</td>\n",
       "      <td>0.995725</td>\n",
       "      <td>0.993559</td>\n",
       "      <td>1.001117</td>\n",
       "      <td>1.002221</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001180</td>\n",
       "      <td>0.971865</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>1.029085</td>\n",
       "      <td>0.979954</td>\n",
       "      <td>0.994330</td>\n",
       "      <td>0.990811</td>\n",
       "      <td>1.003728</td>\n",
       "      <td>0.996244</td>\n",
       "      <td>1.101653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.700000</td>\n",
       "      <td>-147.000000</td>\n",
       "      <td>-67.000000</td>\n",
       "      <td>-81.400000</td>\n",
       "      <td>-2.970000</td>\n",
       "      <td>-9.120000</td>\n",
       "      <td>-2.060000</td>\n",
       "      <td>-1.770000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.940000</td>\n",
       "      <td>-8.410000</td>\n",
       "      <td>-7.460000</td>\n",
       "      <td>-32.900000</td>\n",
       "      <td>-7.780000</td>\n",
       "      <td>-1.220000</td>\n",
       "      <td>-4.560000</td>\n",
       "      <td>-5.150000</td>\n",
       "      <td>-6.180000</td>\n",
       "      <td>-16.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.063700</td>\n",
       "      <td>-0.036950</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.219000</td>\n",
       "      <td>-0.566000</td>\n",
       "      <td>-0.553000</td>\n",
       "      <td>-0.530000</td>\n",
       "      <td>-0.597000</td>\n",
       "      <td>-0.714000</td>\n",
       "      <td>-0.727500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653000</td>\n",
       "      <td>-0.465000</td>\n",
       "      <td>-0.495000</td>\n",
       "      <td>-0.330000</td>\n",
       "      <td>-0.520000</td>\n",
       "      <td>-0.853000</td>\n",
       "      <td>-0.251000</td>\n",
       "      <td>-0.599000</td>\n",
       "      <td>-0.091500</td>\n",
       "      <td>-0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>-0.094200</td>\n",
       "      <td>-0.092300</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>-0.072600</td>\n",
       "      <td>-0.224000</td>\n",
       "      <td>-0.227000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064600</td>\n",
       "      <td>-0.108000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>-0.272000</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>-0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.007340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>40.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       -1.439999999999999891e-01  -1.429999999999999882e-01  \\\n",
       "count               17275.000000               17275.000000   \n",
       "mean                    0.005101                   0.001880   \n",
       "std                     0.147718                   1.159427   \n",
       "min                    -2.700000                -147.000000   \n",
       "25%                    -0.063700                  -0.036950   \n",
       "50%                     0.025600                   0.105000   \n",
       "75%                     0.094800                   0.176000   \n",
       "max                     0.692000                   0.354000   \n",
       "\n",
       "       -1.160000000000000059e-01  -1.029999999999999943e-01  \\\n",
       "count               17275.000000               17275.000000   \n",
       "mean                   -0.002335                  -0.003269   \n",
       "std                     0.993605                   1.055627   \n",
       "min                   -67.000000                 -81.400000   \n",
       "25%                    -0.260000                  -0.219000   \n",
       "50%                     0.092000                   0.082000   \n",
       "75%                     0.361000                   0.312000   \n",
       "max                     1.820000                   1.550000   \n",
       "\n",
       "       2.260000000000000064e-01  2.099999999999999922e-01  \\\n",
       "count              17275.000000              17275.000000   \n",
       "mean                   0.001716                 -0.000099   \n",
       "std                    0.972808                  0.967692   \n",
       "min                   -2.970000                 -9.120000   \n",
       "25%                   -0.566000                 -0.553000   \n",
       "50%                   -0.094200                 -0.092300   \n",
       "75%                    0.454000                  0.438000   \n",
       "max                   24.200000                 23.300000   \n",
       "\n",
       "       -9.799999999999999822e-01  -7.800000000000000266e-01  \\\n",
       "count               17275.000000               17275.000000   \n",
       "mean                    0.002617                  -0.000426   \n",
       "std                     0.995725                   0.993559   \n",
       "min                    -2.060000                  -1.770000   \n",
       "25%                    -0.530000                  -0.597000   \n",
       "50%                     0.014200                  -0.072600   \n",
       "75%                     0.585000                   0.507000   \n",
       "max                    10.100000                  15.200000   \n",
       "\n",
       "       -4.739999999999999769e-01  -4.470000000000000084e-01  \\\n",
       "count               17275.000000               17275.000000   \n",
       "mean                    0.004258                   0.005597   \n",
       "std                     1.001117                   1.002221   \n",
       "min                    -1.570000                  -1.500000   \n",
       "25%                    -0.714000                  -0.727500   \n",
       "50%                    -0.224000                  -0.227000   \n",
       "75%                     0.483000                   0.497000   \n",
       "max                    15.300000                  11.500000   \n",
       "\n",
       "                 ...             -7.379999999999999893e-01  \\\n",
       "count            ...                          17275.000000   \n",
       "mean             ...                              0.000868   \n",
       "std              ...                              1.001180   \n",
       "min              ...                             -1.940000   \n",
       "25%              ...                             -0.653000   \n",
       "50%              ...                             -0.064600   \n",
       "75%              ...                              0.541000   \n",
       "max              ...                             11.700000   \n",
       "\n",
       "       9.250000000000000444e-01  5.160000000000000142e-01  \\\n",
       "count              17275.000000              17275.000000   \n",
       "mean                  -0.006485                 -0.002719   \n",
       "std                    0.971865                  0.997924   \n",
       "min                   -8.410000                 -7.460000   \n",
       "25%                   -0.465000                 -0.495000   \n",
       "50%                   -0.108000                  0.165000   \n",
       "75%                    0.320000                  0.560000   \n",
       "max                   10.700000                  5.500000   \n",
       "\n",
       "       3.439999999999999725e-01  9.060000000000000275e-01  \\\n",
       "count              17275.000000              17275.000000   \n",
       "mean                  -0.005862                 -0.005302   \n",
       "std                    1.029085                  0.979954   \n",
       "min                  -32.900000                 -7.780000   \n",
       "25%                   -0.330000                 -0.520000   \n",
       "50%                    0.203000                 -0.272000   \n",
       "75%                    0.399000                  0.127000   \n",
       "max                   12.400000                  8.950000   \n",
       "\n",
       "       -1.129999999999999893e+00  -5.520000000000000462e-01  \\\n",
       "count               17275.000000               17275.000000   \n",
       "mean                   -0.001169                   0.008673   \n",
       "std                     0.994330                   0.990811   \n",
       "min                    -1.220000                  -4.560000   \n",
       "25%                    -0.853000                  -0.251000   \n",
       "50%                    -0.130000                   0.325000   \n",
       "75%                     0.643000                   0.646000   \n",
       "max                    17.700000                   1.410000   \n",
       "\n",
       "       5.530000000000000471e-01  -4.169999999999999818e-01  \\\n",
       "count              17275.000000               17275.000000   \n",
       "mean                   0.004028                   0.005097   \n",
       "std                    1.003728                   0.996244   \n",
       "min                   -5.150000                  -6.180000   \n",
       "25%                   -0.599000                  -0.091500   \n",
       "50%                    0.280000                   0.328000   \n",
       "75%                    0.805000                   0.585000   \n",
       "max                    1.420000                   1.420000   \n",
       "\n",
       "       2.560000000000000053e-01  \n",
       "count              17275.000000  \n",
       "mean                   0.005348  \n",
       "std                    1.101653  \n",
       "min                  -16.900000  \n",
       "25%                   -0.257000  \n",
       "50%                   -0.142000  \n",
       "75%                    0.007340  \n",
       "max                   40.600000  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier on test set has accuracy: 0.995253256151\n"
     ]
    }
   ],
   "source": [
    "reg_Cs = np.hstack((10.**np.arange(-5, 0), 10**np.arange(0, 6)))\n",
    "\n",
    "logit = LogisticRegressionCV(reg_Cs, cv=5)\n",
    "logit.fit(X_train, y_train)\n",
    "print(\"Logistic regression classifier on test set has accuracy:\", logit.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classifier that predicts 'normal' (label 0) on patients in the test set has accuracy: 0.994674384949\n"
     ]
    }
   ],
   "source": [
    "y_test.values\n",
    "test_zeros = (y_test.values == 0).sum()\n",
    "test_ones = (y_test.values == 1).sum()\n",
    "test_total = y_test.values.shape[0]\n",
    "# test_zeros, test_ones, test_zeros+test_ones, test_total\n",
    "all_zero_accuracy = test_zeros/test_total\n",
    "print(\"A classifier that predicts 'normal' (label 0) on patients in the test set has accuracy:\", all_zero_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 2.A. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients?\n",
    "- Using logistic regression, we get accuracy score on test set 99.53%, whereas using a \"zero for all\" classifier, we get test score of 99.47%.\n",
    "- We can see that these two scores are very close.\n",
    "\n",
    "### 2.B. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "- No, the difference between the classification accuracies are not large at all. So we cannot declare logistic regression is a better classfier than the all 0's classfier, if we only look at the two scores.\n",
    "- This is because the dataset contains more than 69000 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. So the ratio of people who does not have breast cancer is extremely higher than those with malignant tumor. The class labels are very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for the fitted logistic regression classifier:\n",
      "[[17181     2]\n",
      " [   80    12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix for the fitted logistic regression classifier:\")\n",
    "print(confusion_matrix(y_test,logit.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for the all-zero classifier:\n",
      "[[17183     0]\n",
      " [   92     0]]\n"
     ]
    }
   ],
   "source": [
    "all_zero_confusion = np.array([[test_zeros, 0], [test_ones, 0]])\n",
    "print(\"Confusion matrix for the all-zero classifier:\")\n",
    "print(all_zero_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 4. Using the entries of the confusion table compute the true positive rate and the true negative rate for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.\n",
    "\n",
    "#### Logistic Regression Classifier:\n",
    "- True Positive Rate = $\\frac{TP}{TP+FN} = \\frac{12}{12+80} = 13.04\\%$\n",
    "- True Negative Rate = $\\frac{TN}{TN+FP} = \\frac{17181}{17181+2} = 99.99\\%$\n",
    "\n",
    "#### All-zero Classifier:\n",
    "- True Positive Rate = $\\frac{TP}{TP+FN} = \\frac{0}{92+0} = 0\\%$\n",
    "- True Negative Rate = $\\frac{TN}{TN+FP} = \\frac{17183}{17183+0} = 100\\%$\n",
    "\n",
    "- True positive rate measures the proportion of positives that are correctly identified as such. In this case, true positive rate means the percentage of people with breast cancer who are correctly identified as having the condition.\n",
    "- True negative rate measures the proportion of negatives that are correctly identified as such. In this case, true negative rate means the percentage of healthy people who are correctly identified as not having breast cancer.\n",
    "\n",
    "- In this medical test condition, higher true positive rate is more important, because then we will be less likely to miss people who has breast cancer by telling them everything is fine. \n",
    "- Since all-zero classifier tells everybody that they are healthy, it is very useless in this medical area. \n",
    "- Therefore, the fitted logistic regression model is better thant the all 0's classifier.\n",
    "\n",
    "### 5. What is the false positive rate of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?\n",
    "- False Positive Rate of the fitted Logistic Regression Classfier = $\\frac{FP}{FP+TN} = \\frac{2}{2+17181} = 0.01\\%$\n",
    "- False Positive Rate = 1 - True Negative Rate\n",
    "- False positive rate cannot be derived directly from true positive rate.\n",
    "- For a caner detection task, high false positive rate is undesirable because it means false alarms, which may raise unnecessary worry and distress for healthy people. Those healthy people who get false positive results will go to hospital to do more tests, which is a waste of money and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: ROC Analysis\n",
    "\n",
    "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
    "\n",
    "\n",
    "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "\n",
    "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "    - FPR = 0\n",
    "    - FPR = 0.1\n",
    "    - FPR = 0.5\n",
    "    - FPR = 0.9\n",
    "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
    "\n",
    "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
    "\n",
    "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvYAAAHuCAYAAAD0jEhLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRGqA/w8eBUwoCuh0GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1361d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make_roc function from lab 7\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.1', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 3))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.plot(0,0,'ro', label=\"all-zero\")\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "plt.figure(figsize=(12,7.5))\n",
    "ax=make_roc(\"logistic\",logit, y_test, X_test, labe=10, skip=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 1. Display the ROC curve for the fitted classifier on the test set. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "- The ROC curve for logistic is clearly much better than the one for all-zero classifier, which covers about 0.93 area. \n",
    "- The all o's classifier only gives a single point at (0,0), because it predicts everything to be negative, and hence both FPR and TPR will always be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If FPR=0, the highest TPR is 0.010869565217391304, and the threshold is 0.9414417739229958\n",
      "If FPR=0.1, the highest TPR is 0.8369565217391305, and the threshold is 0.009646251614429753\n",
      "If FPR=0.5, the highest TPR is 0.9891304347826086, and the threshold is 0.0008238056503600531\n",
      "If FPR=0.9, the highest TPR is 1.0, and the threshold is 2.155053140078295e-34\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds=roc_curve(y_test, logit.predict_proba(X_test)[:,1])\n",
    "# fpr.shape, tpr.shape, thresholds.shape\n",
    "print(\"If FPR=0, the highest TPR is {}, and the threshold is {}\"\\\n",
    "      .format(tpr[(np.abs(fpr-0)).argmin()], thresholds[(np.abs(fpr-0)).argmin()]))\n",
    "print(\"If FPR=0.1, the highest TPR is {}, and the threshold is {}\"\\\n",
    "      .format(tpr[(np.abs(fpr-0.1)).argmin()], thresholds[(np.abs(fpr-0.1)).argmin()]))\n",
    "print(\"If FPR=0.5, the highest TPR is {}, and the threshold is {}\"\\\n",
    "      .format(tpr[(np.abs(fpr-0.5)).argmin()], thresholds[(np.abs(fpr-0.5)).argmin()]))\n",
    "print(\"If FPR=0.9, the highest TPR is {}, and the threshold is {}\"\\\n",
    "      .format(tpr[(np.abs(fpr-0.9)).argmin()], thresholds[(np.abs(fpr-0.9)).argmin()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 2. Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "- As we decrease the threshold from 1 to 0, the flase positive rate will increase, but we will also get a higher true positive rate. The change in threshold causes a trade-off between FPR and TPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 3. Suppose a clinician told you that diagnosing a cancer patient as normal is twice as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold?\n",
    "- Based on the clinician's point of view, a false negative is twice as critical an error as a false positive.\n",
    "- Therefore, I will recommend trading a higher false positive rate for a higher true positive rate.\n",
    "- My way to interpret twice is as the follows: Both FPR and non-TPR have some cost, but cost for non-TPR is twice more expensive. Therefore, we want to trade FPR for TPR, and the relation between them is about 2*FPR = 1-TPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold=0.01427641549682725, TPR is 0.8369565217391305, and FPR is 0.06832334283885236\n"
     ]
    }
   ],
   "source": [
    "# print(\"For threshold=0.01, TPR is {}, and FPR is {}\"\\\n",
    "#       .format(tpr[(np.abs(thresholds-0.01)).argmin()], fpr[(np.abs(thresholds-0.01)).argmin()]))\n",
    "tpr_penalty = [1-x for x in tpr]\n",
    "fpr_penalty = [x for x in fpr]\n",
    "dif = []\n",
    "for i, j in zip(tpr_penalty, fpr_penalty):\n",
    "    d = abs(i - 2*j)\n",
    "    dif.append(d)\n",
    "min_index = dif.index(min(dif))\n",
    "# print(min_index)\n",
    "# tpr[min_index], fpr[min_index], thresholds[min_index]\n",
    "print(\"For threshold={}, TPR is {}, and FPR is {}\"\\\n",
    "       .format(thresholds[min_index], tpr[min_index], fpr[min_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above calculation, I will recomment a threshold at about 0.014, which gives true positive rate (TPR) at 83.7%, whereas false positive rate (FPR) is at 6.83%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 4. Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the ROC curve for the fitted logistic classifier is: 0.945432037226\n",
      "The area under the ROC curve for the all 0's classifier is: 0.5\n"
     ]
    }
   ],
   "source": [
    "logit_auc = roc_auc_score(y_test, logit.predict_proba(X_test)[:,1])\n",
    "print(\"The area under the ROC curve for the fitted logistic classifier is:\", logit_auc)\n",
    "print(\"The area under the ROC curve for the all 0's classifier is: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3: Missing data\n",
    "\n",
    "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
    "\n",
    "\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
    "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24999, 118)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('HW6_dataset_missing.csv', index_col=0)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 118)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_drop = df2.dropna(axis=0)\n",
    "df2_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1077, 117), (1077,), (359, 117), (359,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = df2_drop.iloc[:, :-1]\n",
    "y2 = df2_drop.iloc[:, -1]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.25, random_state=42)\n",
    "X_train2.shape, y_train2.shape, X_test2.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier on test set with missing values removed has accuracy: 1.0\n",
      "Confusion matrix for the fitted logistic regression classifier:\n",
      "[[359]]\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegressionCV(reg_Cs, cv=3) #data size after removing missing values is small; use cv=3 instead\n",
    "logit.fit(X_train2, y_train2)\n",
    "print(\"Logistic regression classifier on test set with missing values removed has accuracy:\",\\\n",
    "      logit.score(X_test2, y_test2))\n",
    "print(\"Confusion matrix for the fitted logistic regression classifier:\")\n",
    "print(confusion_matrix(y_test2,logit.predict(X_test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 1. Remove all observations that contain and missing values. Report the overall classification rate and TPR in the test set.\n",
    "- With missing values removed, the TPR calculated from the above confusion matrix is N/A, because there is no positive case, and hence zero divide by zero is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24999, 118)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2.copy()\n",
    "for col in df3:\n",
    "    df3[col] = df2[col].fillna(df2[col].mean())\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18749, 117), (18749,), (6250, 117), (6250,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = df3.iloc[:, :-1]\n",
    "y3 = df3.iloc[:, -1]\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.25, random_state=42)\n",
    "X_train3.shape, y_train3.shape, X_test3.shape, y_test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier on test set with missing values imputed has accuracy: 0.99488\n",
      "Confusion matrix for the fitted logistic regression classifier:\n",
      "[[6212    1]\n",
      " [  31    6]]\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegressionCV(reg_Cs, cv=5)\n",
    "logit.fit(X_train3, y_train3)\n",
    "print(\"Logistic regression classifier on test set with missing values imputed has accuracy:\",\\\n",
    "      logit.score(X_test3, y_test3))\n",
    "print(\"Confusion matrix for the fitted logistic regression classifier:\")\n",
    "print(confusion_matrix(y_test3,logit.predict(X_test3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 2. Restart with a fresh copy and impute the missing data via mean imputation. Report the overall classification rate and TPR in the test set.\n",
    "- TPR = $\\frac{TP}{TP+FN} = \\frac{6}{6+31} = 16.22\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns that have missing values are:  ['93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n"
     ]
    }
   ],
   "source": [
    "df4 = df2.copy()\n",
    "nan_cols = df4.columns[df4.isnull().any()].tolist()\n",
    "print(\"columns that have missing values are: \", nan_cols)\n",
    "# df4.isnull().sum()\n",
    "# df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indices = df4['93'].index[df4['93'].apply(np.isnan)].tolist()\n",
    "# indices[0]\n",
    "# df4.iloc[8,92]\n",
    "# np.isnan(df4.iloc[8,92])\n",
    "# inds = df4['93'].index[df4['93'].apply(np.isnan)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use knn with k=5\n",
    "# randomly select from the nearest next 5 neighbors with equal probability\n",
    "# Run this method three times till no more nan values\n",
    "for t in range(3):\n",
    "    for j in nan_cols:\n",
    "        inds = df4[j].index[df4[j].apply(np.isnan)].tolist()\n",
    "        j_int = int(j)-1\n",
    "        for i in inds:\n",
    "            if i+6 >= df4.shape[0]:\n",
    "                df4.iloc[i,j_int] = df4.iloc[0,j_int]\n",
    "            else:\n",
    "                random_number = np.random.random()\n",
    "                if random_number < 0.2:\n",
    "                    if not np.isnan(df4.iloc[i+1, j_int]):\n",
    "                        df4.iloc[i, j_int] = df4.iloc[i+1, j_int]\n",
    "                    else:\n",
    "                        df4.iloc[i, j_int] = df4.iloc[i+2, j_int]\n",
    "                elif random_number >= 0.2 and random_number < 0.4:\n",
    "                    if not np.isnan(df4.iloc[i+2, j_int]):\n",
    "                        df4.iloc[i,j_int] = df4.iloc[i+2, j_int]\n",
    "                    else:\n",
    "                        df4.iloc[i, j_int] = df4.iloc[i+3, j_int]\n",
    "                elif random_number >= 0.4 and random_number < 0.6:\n",
    "                    if not np.isnan(df4.iloc[i+3, j_int]):\n",
    "                        df4.iloc[i,j_int] = df4.iloc[i+3, j_int]\n",
    "                    else:\n",
    "                        df4.iloc[i, j_int] = df4.iloc[i+4, j_int]\n",
    "                elif random_number >= 0.6 and random_number < 0.8:\n",
    "                    if not np.isnan(df4.iloc[i+4, j_int]):\n",
    "                        df4.iloc[i,j_int] = df4.iloc[i+4, j_int]\n",
    "                    else:\n",
    "                        df4.iloc[i, j_int] = df4.iloc[i+5, j_int]\n",
    "                elif random_number >= 0.8:\n",
    "                    if not np.isnan(df4.iloc[i+5, j_int]):\n",
    "                        df4.iloc[i,j_int] = df4.iloc[i+5, j_int] \n",
    "                    else:\n",
    "                        df4.iloc[i, j_int] = df4.iloc[i+6, j_int]\n",
    "\n",
    "# check if there is any more nan values: want to see an empty list\n",
    "df4.columns[df4.isnull().any()].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18749, 117), (18749,), (6250, 117), (6250,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4 = df4.iloc[:, :-1]\n",
    "y4 = df4.iloc[:, -1]\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y4, test_size=0.25, random_state=42)\n",
    "X_train4.shape, y_train4.shape, X_test4.shape, y_test4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier on test set with missing values using knn model imputation has accuracy: 0.99504\n",
      "Confusion matrix for the fitted logistic regression classifier:\n",
      "[[6211    2]\n",
      " [  29    8]]\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegressionCV(reg_Cs, cv=5)\n",
    "logit.fit(X_train4, y_train4)\n",
    "print(\"Logistic regression classifier on test set with missing values using knn model imputation has accuracy:\",\\\n",
    "      logit.score(X_test4, y_test4))\n",
    "print(\"Confusion matrix for the fitted logistic regression classifier:\")\n",
    "print(confusion_matrix(y_test4,logit.predict(X_test4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 3. Again restart with a fresh copy and impute the missing data via a model-based imputation method. Report the overall classification rate and TPR in the test set.\n",
    "- TPR = $\\frac{TP}{TP+FN} = \\frac{8}{8+29} = 21.62\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 4. Compare the results in the 3 previous parts of this problem. Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "With missing values present in the dataset, we tried 3 methods: removing missing values, mean imputation and model-based (I chose knn model with k=5) imputation. In this problem, it is not a good idea to completely remove missing values, because doing this will significantly shrink the size of the dataset, and we can see then we have no more observations with patient who have breast cancer, and hence we cannot get any TPR information. Comparing the TPR in the test set, we can see that model-based imputation gives the highest true positive rate. This is because using model-based imputation plus some uncertainty, the value imputed will be more likely to represent real data. However, the computational complexity of the model-based imputation is much greater than the other two, and the TPR result is not that much higher than simple mean imputation. Therefore, I prefer use mean imputation in this problem than the other two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
    "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
    "Define the individual log-likelihood for each observation $i$ as\n",
    "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
    "with linear predictor\n",
    "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
    "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
    "\n",
    "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
    "\n",
    "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
    "\n",
    "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
    "\n",
    "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (\\emph{Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
    "\n",
    "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
    "\n",
    "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
    "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
    "That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
    "\n",
    "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
    "\n",
    "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "### 1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$l(\\theta) = \\sum_{i=1}^{n}l_i (\\theta_i)\n",
    "= \\sum_{i=1}^{n}log(h(y_i)) + \\sum_{i=1}^{n}[y_i \\theta_i - b(\\theta_i)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. . Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l_i}{\\partial \\beta_j}$ = $\\frac{\\partial l_i}{\\partial \\theta_i} \\cdot\n",
    "\\frac{\\partial \\theta_i}{\\partial \\mu_i} \\cdot\n",
    "\\frac{\\partial \\mu_i}{\\partial \\eta_i} \\cdot\n",
    "\\frac{\\partial \\eta_i}{\\partial \\beta_j}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l_i}{\\partial \\theta_i} = y_i - b'(\\theta_i) = y_i - \\mu_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\eta_i = x_i^T\\beta = \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}$, we get: $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (Hint: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu_i = b'(\\theta_i)$\n",
    "\n",
    "$\\frac{\\partial \\theta_i}{\\partial \\mu_i} = (\\frac{\\partial \\mu_i}{\\partial \\theta_i})^{-1}\n",
    "= b''(\\theta_i)^{-1} = (Var(y_i|\\theta_i))^{-1} = \\frac{1}{Var(y_i|\\theta_i)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta_i = g(\\mu_i)$\n",
    "\n",
    "$\\frac{\\partial \\mu_i}{\\partial \\eta_i} = (\\frac{\\partial \\eta_i}{\\partial \\mu_i})^{-1}\n",
    "= g'(\\mu_i)^{-1} = \\frac{1}{g'(\\mu_i)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
    "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
    "That is, compute the matrices $D$ and $V$ such that this equation holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l_i}{\\partial \\beta_j} = \\frac{\\partial l_i}{\\partial \\theta_i} \\cdot\n",
    "\\frac{\\partial \\theta_i}{\\partial \\mu_i} \\cdot\n",
    "\\frac{\\partial \\mu_i}{\\partial \\eta_i} \\cdot\n",
    "\\frac{\\partial \\eta_i}{\\partial \\beta_j}\n",
    "= (y_i - \\mu_i) \\cdot \\frac{1}{Var(y_i|\\theta_i)} \\cdot \\frac{1}{g'(\\mu_i)} \\cdot x_{ij}\n",
    "$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\beta_j}\n",
    "= \\sum_{i=1}^{n}(y_i - \\mu_i) \\cdot \\frac{1}{Var(y_i|\\theta_i)} \\cdot \\frac{1}{g'(\\mu_i)} \\cdot x_{ij}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matrix form, y and $\\mu$ are column vectors, and X is the data matrix.\n",
    "\n",
    "If we write this expression using matrix notation as $\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$, we will get the following matrices $D$ and $V^{-1}$:\n",
    "\n",
    "$D = \n",
    "\\begin{bmatrix}\n",
    "g'(\\mu_1)^{-1} & 0 & \\cdots & 0 \\\\\n",
    "0 & g'(\\mu_2)^{-1} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & g'(\\mu_n)^{-1}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$V =\n",
    "\\begin{bmatrix}\n",
    "Var(y_1|\\theta_1) & 0 & \\cdots & 0 \\\\\n",
    "0 & Var(y_2|\\theta_2) & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & Var(y_n|\\theta_n)\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Note that $V^{-1}$ is just:\n",
    "$V^{-1}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{Var(y_1|\\theta_1)} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\frac{1}{Var(y_2|\\theta_2)} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\frac{1}{Var(y_n|\\theta_n)}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "in the previous equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. If we use the canonical link function, how do your answers to part (6) simplify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canonical link: $\\theta = \\eta$\n",
    "\n",
    "Therefore, we have $\\frac{\\partial \\theta_i}{\\partial \\mu_i} \\cdot \\frac{\\partial \\mu_i}{\\partial \\eta_i} = 1$\n",
    "\n",
    "The answer to part (6) can be simplified as $\\nabla_{\\beta} l(\\beta) = X(Y - \\mu) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the above likelihood equation, we get $Y=\\mu$, since $X$ cannot be zero.\n",
    "\n",
    "In the case of logistic regression, we have $Y|\\mu \\sim Bernoulli(\\mu)$ and therefore:\n",
    "\n",
    "$f(y|\\mu) = \\mu^y (1-\\mu)^{1-y}\n",
    "= e^{ylog(\\mu) + (1-y)log(1-\\mu)}\n",
    "= e^{ylog(\\frac{\\mu}{1-\\mu}) + log(1-\\mu)}\n",
    "$\n",
    "\n",
    "Therefore, we have $\\theta = log(\\frac{\\mu}{1-\\mu}) = log(\\frac{Y}{1-Y})$.\n",
    "\n",
    "So $\\frac{Y}{1-Y} = e^\\theta = e^{X^T\\beta}$, using the canonical link.\n",
    "\n",
    "And we get $Y=\\frac{e^{X^T\\beta}}{1+e^{X^T\\beta}}$, which is the solution of $P(Y=1)$ given in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
