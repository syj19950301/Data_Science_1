{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 8\n",
    "# Ensemble methods\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between a collision that produces Higgs bosons and collisions thats produce only background noise. We shall explore the use of ensemble methods for this classification task.\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle colision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background). \n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: <a href = \"https://www.nature.com/articles/ncomms5308\">Baldi et al., Nature Communications 5, 2014</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2pt): Single Decision Tree\n",
    "We start by building a basic model which we will use as our base model for comparison. \n",
    "\n",
    "1. Fit a decision tree model to the training set and report the classification accuracy of the model on the test set. Use 5-fold cross-validation to choose the (maximum) depth for the tree. You will use the max_depth you find here throughout the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>_lepton_eta</th>\n",
       "      <th>_lepton_phi</th>\n",
       "      <th>_missing_energy_magnitude</th>\n",
       "      <th>_missing_energy_phi</th>\n",
       "      <th>_jet_1_pt</th>\n",
       "      <th>_jet_1_eta</th>\n",
       "      <th>_jet_1_phi</th>\n",
       "      <th>_jet_1_b-tag</th>\n",
       "      <th>_jet_2_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>_jet_4_phi</th>\n",
       "      <th>_jet_4_b-tag</th>\n",
       "      <th>_m_jj</th>\n",
       "      <th>_m_jjj</th>\n",
       "      <th>_m_lv</th>\n",
       "      <th>_m_jlv</th>\n",
       "      <th>_m_bb</th>\n",
       "      <th>_m_wbb</th>\n",
       "      <th>_m_wwbb</th>\n",
       "      <th>_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376816</td>\n",
       "      <td>-1.583727</td>\n",
       "      <td>-1.707552</td>\n",
       "      <td>0.990897</td>\n",
       "      <td>0.114397</td>\n",
       "      <td>1.253553</td>\n",
       "      <td>0.619859</td>\n",
       "      <td>-1.479572</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.753658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522449</td>\n",
       "      <td>1.318622</td>\n",
       "      <td>0.982398</td>\n",
       "      <td>1.359610</td>\n",
       "      <td>0.964809</td>\n",
       "      <td>1.309991</td>\n",
       "      <td>1.083203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707330</td>\n",
       "      <td>0.087603</td>\n",
       "      <td>-0.399742</td>\n",
       "      <td>0.918742</td>\n",
       "      <td>-1.229936</td>\n",
       "      <td>1.172847</td>\n",
       "      <td>-0.552574</td>\n",
       "      <td>0.886053</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>1.298317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439696</td>\n",
       "      <td>0.828885</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>1.157820</td>\n",
       "      <td>2.215780</td>\n",
       "      <td>1.189586</td>\n",
       "      <td>0.937976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617290</td>\n",
       "      <td>0.265839</td>\n",
       "      <td>-1.345227</td>\n",
       "      <td>1.154581</td>\n",
       "      <td>1.036646</td>\n",
       "      <td>0.954822</td>\n",
       "      <td>0.377252</td>\n",
       "      <td>-0.147960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.024506</td>\n",
       "      <td>1.026255</td>\n",
       "      <td>0.986289</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>1.371080</td>\n",
       "      <td>0.981672</td>\n",
       "      <td>0.917436</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850992</td>\n",
       "      <td>-0.380876</td>\n",
       "      <td>-0.071264</td>\n",
       "      <td>1.468704</td>\n",
       "      <td>-0.795133</td>\n",
       "      <td>0.691818</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>0.496881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.616349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.520171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.197755</td>\n",
       "      <td>1.100534</td>\n",
       "      <td>0.987262</td>\n",
       "      <td>1.353453</td>\n",
       "      <td>1.455383</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>0.953553</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.767540</td>\n",
       "      <td>-0.691572</td>\n",
       "      <td>-0.040191</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.143765</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.397057</td>\n",
       "      <td>-0.873640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.147862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502034</td>\n",
       "      <td>1.550981</td>\n",
       "      <td>0.921948</td>\n",
       "      <td>0.864080</td>\n",
       "      <td>0.982839</td>\n",
       "      <td>1.373222</td>\n",
       "      <td>0.601492</td>\n",
       "      <td>0.918621</td>\n",
       "      <td>0.957063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton_pT  _lepton_eta  _lepton_phi  _missing_energy_magnitude  \\\n",
       "0   0.376816    -1.583727    -1.707552                   0.990897   \n",
       "1   0.707330     0.087603    -0.399742                   0.918742   \n",
       "2   0.617290     0.265839    -1.345227                   1.154581   \n",
       "3   0.850992    -0.380876    -0.071264                   1.468704   \n",
       "4   0.767540    -0.691572    -0.040191                   0.614843   \n",
       "\n",
       "   _missing_energy_phi  _jet_1_pt  _jet_1_eta  _jet_1_phi  _jet_1_b-tag  \\\n",
       "0             0.114397   1.253553    0.619859   -1.479572      2.173076   \n",
       "1            -1.229936   1.172847   -0.552574    0.886053      2.173076   \n",
       "2             1.036646   0.954822    0.377252   -0.147960      0.000000   \n",
       "3            -0.795133   0.691818    0.883260    0.496881      0.000000   \n",
       "4             0.143765   0.748614    0.397057   -0.873640      0.000000   \n",
       "\n",
       "   _jet_2_pt   ...    _jet_4_phi  _jet_4_b-tag     _m_jj    _m_jjj     _m_lv  \\\n",
       "0   0.753658   ...      0.397156      0.000000  0.522449  1.318622  0.982398   \n",
       "1   1.298317   ...      0.236231      0.000000  0.439696  0.828885  0.992241   \n",
       "2   1.063507   ...     -0.542413      0.000000  1.024506  1.026255  0.986289   \n",
       "3   1.616349   ...     -1.520171      0.000000  1.197755  1.100534  0.987262   \n",
       "4   1.147862   ...      0.502034      1.550981  0.921948  0.864080  0.982839   \n",
       "\n",
       "     _m_jlv     _m_bb    _m_wbb   _m_wwbb  _class  \n",
       "0  1.359610  0.964809  1.309991  1.083203     1.0  \n",
       "1  1.157820  2.215780  1.189586  0.937976     1.0  \n",
       "2  0.927720  1.371080  0.981672  0.917436     1.0  \n",
       "3  1.353453  1.455383  0.994682  0.953553     1.0  \n",
       "4  1.373222  0.601492  0.918621  0.957063     0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"Higgs_train.csv\")\n",
    "train.columns = train.columns.map(lambda x: x.replace(' ', '_'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>_lepton_eta</th>\n",
       "      <th>_lepton_phi</th>\n",
       "      <th>_missing_energy_magnitude</th>\n",
       "      <th>_missing_energy_phi</th>\n",
       "      <th>_jet_1_pt</th>\n",
       "      <th>_jet_1_eta</th>\n",
       "      <th>_jet_1_phi</th>\n",
       "      <th>_jet_1_b-tag</th>\n",
       "      <th>_jet_2_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>_jet_4_phi</th>\n",
       "      <th>_jet_4_b-tag</th>\n",
       "      <th>_m_jj</th>\n",
       "      <th>_m_jjj</th>\n",
       "      <th>_m_lv</th>\n",
       "      <th>_m_jlv</th>\n",
       "      <th>_m_bb</th>\n",
       "      <th>_m_wbb</th>\n",
       "      <th>_m_wwbb</th>\n",
       "      <th>_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.883751</td>\n",
       "      <td>-0.461715</td>\n",
       "      <td>0.196283</td>\n",
       "      <td>1.330217</td>\n",
       "      <td>1.522737</td>\n",
       "      <td>1.039467</td>\n",
       "      <td>-1.519038</td>\n",
       "      <td>-1.457952</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.137282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.454688</td>\n",
       "      <td>0.790714</td>\n",
       "      <td>1.397708</td>\n",
       "      <td>1.250985</td>\n",
       "      <td>0.712875</td>\n",
       "      <td>0.811940</td>\n",
       "      <td>0.820693</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780168</td>\n",
       "      <td>-0.292245</td>\n",
       "      <td>0.897072</td>\n",
       "      <td>0.384474</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>1.215078</td>\n",
       "      <td>-0.466424</td>\n",
       "      <td>-0.920208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.104803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812113</td>\n",
       "      <td>0.727805</td>\n",
       "      <td>0.975326</td>\n",
       "      <td>0.636811</td>\n",
       "      <td>0.569234</td>\n",
       "      <td>0.776607</td>\n",
       "      <td>0.715494</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.352659</td>\n",
       "      <td>-1.073368</td>\n",
       "      <td>-1.741953</td>\n",
       "      <td>1.174700</td>\n",
       "      <td>-0.198694</td>\n",
       "      <td>0.557980</td>\n",
       "      <td>0.057408</td>\n",
       "      <td>-1.491768</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.545824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829461</td>\n",
       "      <td>1.060325</td>\n",
       "      <td>0.992326</td>\n",
       "      <td>0.824898</td>\n",
       "      <td>0.365448</td>\n",
       "      <td>0.800015</td>\n",
       "      <td>0.765989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.757292</td>\n",
       "      <td>0.821975</td>\n",
       "      <td>-1.290851</td>\n",
       "      <td>0.207558</td>\n",
       "      <td>-0.150971</td>\n",
       "      <td>1.222224</td>\n",
       "      <td>-1.638856</td>\n",
       "      <td>1.531346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.683582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116925</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>4.290139</td>\n",
       "      <td>2.415188</td>\n",
       "      <td>0.994889</td>\n",
       "      <td>0.923447</td>\n",
       "      <td>0.927035</td>\n",
       "      <td>1.755831</td>\n",
       "      <td>1.362970</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.038721</td>\n",
       "      <td>2.025801</td>\n",
       "      <td>-0.471873</td>\n",
       "      <td>0.423674</td>\n",
       "      <td>-1.497848</td>\n",
       "      <td>1.062186</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>1.218678</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.805279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079062</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.894990</td>\n",
       "      <td>0.936199</td>\n",
       "      <td>1.027161</td>\n",
       "      <td>1.559567</td>\n",
       "      <td>1.148236</td>\n",
       "      <td>1.115536</td>\n",
       "      <td>1.157044</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton_pT  _lepton_eta  _lepton_phi  _missing_energy_magnitude  \\\n",
       "0   0.883751    -0.461715     0.196283                   1.330217   \n",
       "1   0.780168    -0.292245     0.897072                   0.384474   \n",
       "2   0.352659    -1.073368    -1.741953                   1.174700   \n",
       "3   0.757292     0.821975    -1.290851                   0.207558   \n",
       "4   2.038721     2.025801    -0.471873                   0.423674   \n",
       "\n",
       "   _missing_energy_phi  _jet_1_pt  _jet_1_eta  _jet_1_phi  _jet_1_b-tag  \\\n",
       "0             1.522737   1.039467   -1.519038   -1.457952      2.173076   \n",
       "1             0.412700   1.215078   -0.466424   -0.920208      0.000000   \n",
       "2            -0.198694   0.557980    0.057408   -1.491768      1.086538   \n",
       "3            -0.150971   1.222224   -1.638856    1.531346      0.000000   \n",
       "4            -1.497848   1.062186    0.798100    1.218678      2.173076   \n",
       "\n",
       "   _jet_2_pt   ...    _jet_4_phi  _jet_4_b-tag     _m_jj    _m_jjj     _m_lv  \\\n",
       "0   0.361469   ...     -1.137282      0.000000  1.454688  0.790714  1.397708   \n",
       "1   1.104803   ...     -0.091825      0.000000  0.812113  0.727805  0.975326   \n",
       "2   0.911667   ...      1.545824      0.000000  0.829461  1.060325  0.992326   \n",
       "3   1.683582   ...      0.116925      3.101961  4.290139  2.415188  0.994889   \n",
       "4   0.805279   ...     -0.079062      3.101961  0.894990  0.936199  1.027161   \n",
       "\n",
       "     _m_jlv     _m_bb    _m_wbb   _m_wwbb  _class  \n",
       "0  1.250985  0.712875  0.811940  0.820693     1.0  \n",
       "1  0.636811  0.569234  0.776607  0.715494     1.0  \n",
       "2  0.824898  0.365448  0.800015  0.765989     0.0  \n",
       "3  0.923447  0.927035  1.755831  1.362970     1.0  \n",
       "4  1.559567  1.148236  1.115536  1.157044     1.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"Higgs_test.csv\")\n",
    "test.columns = test.columns.map(lambda x: x.replace(' ', '_'))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For depth of 2, 3, ..., 10:\n",
      "train scores are: \n",
      " [0.63859999999999995, 0.64219999999999999, 0.66020000000000001, 0.68200000000000005, 0.71060000000000001, 0.74280000000000002, 0.7752, 0.81579999999999997, 0.8468]\n",
      "test scores are: \n",
      " [0.64300000000000002, 0.64319999999999999, 0.64959999999999996, 0.64559999999999995, 0.65620000000000001, 0.64759999999999995, 0.62739999999999996, 0.62160000000000004, 0.62539999999999996]\n"
     ]
    }
   ],
   "source": [
    "depths = [i for i in range(2, 11)]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for depth in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth = depth)\n",
    "    clf.fit(train.iloc[:,:-1],train.iloc[:,-1])\n",
    "    train_scores.append(clf.score(train.iloc[:,:-1],train.iloc[:,-1]))\n",
    "    test_scores.append(clf.score(test.iloc[:,:-1],test.iloc[:,-1]))\n",
    "print(\"For depth of 2, 3, ..., 10:\")\n",
    "print(\"train scores are: \\n\", train_scores)\n",
    "print(\"test scores are: \\n\", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.62320749580749579),\n",
       " (3, 0.62041149381149385),\n",
       " (4, 0.63580570720570717),\n",
       " (5, 0.64001170881170888),\n",
       " (6, 0.62680570360570353),\n",
       " (7, 0.629007704007704),\n",
       " (8, 0.62720070060070054),\n",
       " (9, 0.62540750100750098),\n",
       " (10, 0.61760429580429577)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_scores = []\n",
    "for depth in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth = depth)  \n",
    "    # Perform 5-fold cross validation \n",
    "    score = cross_val_score(estimator=clf, X=train.iloc[:,:-1], y=train.iloc[:,-1], cv=5)\n",
    "    total_scores.append((depth, np.mean(score)))\n",
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The depth of the tree using 5-fold cross-validation is: 5\n"
     ]
    }
   ],
   "source": [
    "# max(total_scores, key=lambda x:x[1])\n",
    "best_depth = max(total_scores, key=lambda x:x[1])[0]\n",
    "print(\"The depth of the tree using 5-fold cross-validation is:\", best_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (15pt): Dropout-based Approach\n",
    "We start with a simple method inspired from the idea of 'dropout' in machine learning, where we fit multiple decision trees on random subsets of predictors, and combine them through a majority vote. The procedure is described below.\n",
    "\n",
    "- For each predictor in the training sample, set the predictor values to 0 with probability $p$  (i.e. drop the predictor by setting it to 0). Repeat this for $B$ trials to create $B$ separate training sets.\n",
    "\n",
    "\n",
    "- Fit decision tree models $\\hat{h}^1(x), \\ldots, \\hat{h}^B(x) \\in \\{0,1\\}$ to the $B$ training sets. You may allow the trees to have unrestricted depth.\n",
    "\n",
    "- Combine the decision tree models into a single classifier by taking a majority vote:\n",
    "$$\n",
    "\\hat{H}_{maj}(x) \\,=\\, majority\\Big(\\hat{h}^1(x), \\ldots, \\hat{h}^B(x)\\Big).\n",
    "$$\n",
    "\n",
    "\n",
    "We shall refer to the combined classifier as an ** *ensemble classifier* **. Implement the described dropout approach, and answer the following questions:\n",
    "1. Apply the dropout procedure with $p = 0.5$ for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracy of the combined classifier. Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier.\n",
    "- Fix the number of trees to 64 and apply the dropout procedure with different dropout rates $p = 0.1, 0.3, 0.5, 0.7, 0.9$. Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier.\n",
    "- Apply 5-fold cross-validation to choose the optimal combination of the dropout rate and number of trees. How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?\n",
    "[hint: Training with large number of trees can take long time. You may need to restrict the max number of trees.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1: Apply the dropout procedure with $p = 0.5$ for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracy of the combined classifier. Does an increase in the number of trees improve the training and test performance? Explain your observations in terms of the bias-variance trade-off for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2: Fix the number of trees to 64 and apply the dropout procedure with different dropout rates $p = 0.1, 0.3, 0.5, 0.7, 0.9$. Based on your results, explain how the dropout rate influences the bias and variance of the combined classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3: Apply 5-fold cross-validation to choose the optimal combination of the dropout rate and number of trees. How does the test performance of an ensemble of trees fitted with the optimal dropout rate and number of trees compare with the single decision tree model in Question 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (15pt): Random Forests\n",
    "\n",
    "We now move to a more sophisticated ensemble technique, namely random forest:\n",
    "1. How does a random forest approach differ from the dropout procedure described in Question 2? \n",
    " \n",
    "- Fit random forest models to the training set for different number of trees (say $2, 4, 8, 16, \\ldots, 256$), and evaluate the training and test accuracies of the models. You may set the number of predictors for each tree in the random forest model to $\\sqrt{p}$, where $p$ is the total number of predictors. \n",
    "\n",
    "- Based on your results, do you find that a larger number of trees necessarily improves the test accuracy of a random forest model? Explain how the number of trees effects the training and test accuracy of a random forest classifier, and how this relates to the bias-variance trade-off for the classifier. \n",
    "  \n",
    "- Fixing the number of trees to a reasonable value, apply 5-fold cross-validation to choose the optimal value for the  number of predictors. How does the test performance of random forest model fitted with the optimal number of trees compare with the dropout approach in Question 2?  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15pt): Boosting\n",
    "\n",
    "We next compare the random forest model with the approach of boosting:\n",
    "\n",
    "\n",
    "1. Apply the AdaBoost algorithm to fit an ensemble of decision trees. Set the learning rate to 0.05, and try out different tree depths for the base learners: 1, 2, 10, and unrestricted depth.  Make a plot of the training accuracy of the ensemble classifier as a function of tree depths. Make a similar plot of the test accuracies as a function of number of trees (say $2, 4, 8, 16, \\ldots, 256$).\n",
    "- How does the number of trees influence the training and test performance? Compare and contrast between the trends you see in the training and test performance of AdaBoost and that of the random forest models in Question 3. Give an explanation for your observations.\n",
    "- How does the tree depth of the base learner impact the training and test performance? Recall that with random forests, we allow the depth of the individual trees to be unrestricted. Would you recommend the same strategy for boosting? Explain your answer.\n",
    "- Apply 5-fold cross-validation to choose the optimal number of trees $B$ for the ensemble and the optimal tree depth for the base learners. How does an ensemble classifier fitted with the optimal number of trees and the optimal tree depth compare with the random forest model fitted in Question 3.4? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (3pt): Meta-classifier\n",
    "\n",
    "We have so far explored techniques that grow a collection of trees either by creating multiple copies of the original training set, or through a sequential procedure, and then combines these trees into a single classifier. Consider an alternate scenario where you are provided with a pre-trained collection of trees, say from different participants of a data science competition for Higgs boson discovery. What would be a good strategy to combine these pre-fitted trees into a single powerful classifier? Of course, a simple approach would be to take the majority vote from the individual trees. Can we do better than this simple combination strategy?\n",
    "\n",
    "A collection of 100 decision tree classifiers is provided in the file `models.npy` and can be loaded into an array by executing:\n",
    "\n",
    "`models = np.load('models.npy')`\n",
    "\n",
    "You can make predictions using the $i^\\text{th}$ model on an array of predictors `x` by executing:\n",
    "\n",
    "`model[i].predict(x)`  &nbsp;&nbsp;&nbsp;\n",
    "or &nbsp;&nbsp;&nbsp;\n",
    "`model[i].predict_proba(x)`\n",
    "\n",
    "and score the model on predictors `x` and labels `y` by using:\n",
    "\n",
    "`model[i].score(x, y)`.\n",
    "\n",
    "1. Implement a strategy to combine the provided decision tree classifiers, and compare the test perfomance of your approach with the majority vote classifier. Explain your strategy/algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "## APCOMP209a - Homework Question\n",
    "â€‹\n",
    "We've worked with imputation methods on missing data in Homework 6.  We've worked with Decision Trees in HW7 and here.  Now let's see what happens if we try to work with Decision Trees and Missing Data at the same time! We'll be working with a dataset from the UCI Machine Learning Repository that uses a variety of wine chemical predictors to classify wines grown in the same region in Italy.  Each line represents 13 (mostly chemical) predictors of the response variable wine class, including things like alcohol content, hue , and phenols.  Unfortunately some of the predictor values were lost in measurement. Please load `wine_quality_missing.csv`. \n",
    "â€‹\n",
    "*Note*: As in HW6 be careful of reading/treating column names and row names in this data set.\n",
    "â€‹\n",
    "â€‹\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the sklearn DecisionTreeClassifier and RandomForestClassifier.   Use cross-validation to find the optimal tree depth for each method.  Report the optimal tree-depth, overall classification rate and confusion matrix on the test set for each method.\n",
    "2. Restart with a fresh copy of the data and impute the missing data via mean imputation.  Split the data 75-25 and again fit DecisionTreeClassifier and RandomForestClassifier using cross-validation to find the optimal tree depth.  Report the optimal tree depth, overall classification rate and confusion matrix on the test set for each method.  \n",
    "3. Again restart with a fresh copy of the data but this time let's try something different.  As discussed in section, CART Decision Trees can take advantage of surrogate splits to handle missing data.  Split the data 75-25 and construct a **custom** decision tree model and train it on the training set with missing data. Report the optimal tree depth, overall classification rate and confusion matrix on the test set and compare your results to the Imputation and DecisionTree model results in part 1 & 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
